package org.apache.hadoop.mapreduce.lib.output;

import java.io.IOException;
import java.util.Arrays;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.FileUtil;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.MapFile;
import org.apache.hadoop.io.SequenceFile.CompressionType;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.io.compress.CompressionCodec;
import org.apache.hadoop.io.compress.DefaultCodec;
import org.apache.hadoop.mapreduce.Partitioner;
import org.apache.hadoop.mapreduce.RecordWriter;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.hadoop.util.ReflectionUtils;

public class MapFileOutputFormat<T> extends FileOutputFormat<WritableComparable<T>, Writable> {

    @Override
    public RecordWriter<WritableComparable<T>, Writable> getRecordWriter(final TaskAttemptContext context)
            throws IOException, InterruptedException {

        // get the path of the temporary output file
        Path file = getDefaultWorkFile(context, "");
        FileSystem fs = file.getFileSystem(context.getConfiguration());

        CompressionCodec codec = null;
        CompressionType compressionType = CompressionType.NONE;
        if (getCompressOutput(context)) {

            // find the kind of compression to do
            compressionType = SequenceFileOutputFormat.getOutputCompressionType(context);

            // find the right codec
            Class<? extends CompressionCodec> codecClass = getOutputCompressorClass(context, DefaultCodec.class);
            codec = ReflectionUtils.newInstance(codecClass, context.getConfiguration());
        }

        final MapFile.Writer out = createMapFileWriter(context, fs, file, compressionType, codec);

        return new RecordWriter<WritableComparable<T>, Writable>() {

            @Override
            public void close(final TaskAttemptContext context) throws IOException, InterruptedException {
                out.close();
            }

            @Override
            public void write(final WritableComparable<T> key, final Writable value) throws IOException {
                out.append(key, value);
            }
        };
    }

    protected MapFile.Writer createMapFileWriter(final TaskAttemptContext context, final FileSystem fs,
            final Path file, final CompressionType compressionType, final CompressionCodec codec) throws IOException {

        return new MapFile.Writer(context.getConfiguration(), fs, file.toString(), context.getOutputKeyClass()
                .asSubclass(WritableComparable.class), context.getOutputValueClass().asSubclass(Writable.class),
                compressionType, codec, context);
    }

    /**
     * Get an entry from output generated by this class.
     */
    public static <K extends WritableComparable<?>, V extends Writable> Writable getEntry(
            final MapFile.Reader[] readers, final Partitioner<K, V> partitioner, final K key, final V value)
            throws IOException {

        int part = partitioner.getPartition(key, value, readers.length);
        return readers[part].get(key, value);
    }

    /**
     * Open the output generated by this format.
     */
    public static MapFile.Reader[] getReaders(final FileSystem ignored, final Path dir, final Configuration conf)
            throws IOException {

        FileSystem fs = dir.getFileSystem(conf);
        Path[] names = FileUtil.stat2Paths(fs.listStatus(dir));

        // sort names, so that hash partitioning works
        Arrays.sort(names);

        MapFile.Reader[] parts = new MapFile.Reader[names.length];
        for (int i = 0; i < names.length; i++) {
            parts[i] = new MapFile.Reader(fs, names[i].toString(), conf);
        }

        return parts;
    }
}
